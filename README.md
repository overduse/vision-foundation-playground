# Vision Foundation Playground ğŸ§ª

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-green)](./LICENSE)
[![Status](https://img.shields.io/badge/Status-Work_in_Progress-orange)]()

A modular codebase for exploring and experimenting with modern Computer Vision Foundation Models. This repository focuses on **Zero-shot** and **Few-shot** learning applications using state-of-the-art models.

The goal is to build a reusable library (`src`) to quickly prototype different downstream tasks (in `projects`), starting with CLIP and extending to SAM (Segment Anything Model).

## ğŸ—ºï¸ Roadmap

- [x] **Project Structure Setup**: Initialize modular architecture.
- [ ] **CLIP Integration**:
    - Wrap OpenAI/OpenCLIP models for easy inference.
    - Implement Zero-shot Image Classification.
- [ ] **SAM (Segment Anything)**:
    - Integrate SAM for prompt-able segmentation.
    - Implement interactive masking tools.
- [ ] **Advanced Pipelines**:
    - **CLIP + SAM**: Zero-shot semantic segmentation (detect objects with text, segment with SAM).
    - Few-shot adaptation experiments.

## ğŸ“‚ Directory Structure

The repository is organized to separate core logic from experimental scripts:

```text
.
â”œâ”€â”€ configs/          # Configuration files (.yaml) for models/experiments
â”œâ”€â”€ data/             # Dataset storage (Ignored by Git)
â”œâ”€â”€ notebooks/        # Jupyter notebooks for quick exploration & prototyping
â”œâ”€â”€ projects/         # Standalone scripts for specific tasks (e.g., classifier)
â”œâ”€â”€ src/              # Core library code
â”‚   â”œâ”€â”€ models/       # Model wrappers (CLIP, SAM, etc.)
â”‚   â””â”€â”€ utils/        # Shared utilities (Image IO, visualization)
â”œâ”€â”€ requirements.txt  # Python dependencies
â””â”€â”€ README.md         # Project documentation
```

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.
